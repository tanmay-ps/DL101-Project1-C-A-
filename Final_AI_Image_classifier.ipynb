{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyODFCZxG+5CFGpUiRNHktKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanmay-ps/DL101-Project1-C-A-/blob/main/Final_AI_Image_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "0l3jCHo5aYfz",
        "outputId": "4e3b637d-cad9-44cf-926e-50f5dff88873"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pygoogle_image'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3408730494.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpygoogle_image\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pygoogle_image'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "#                                AI IMAGE CLASSIFIER\n",
        "\n",
        "##### This project involved building a machine learning model that can effectively classify real images and those generated by AI. The first step was to create a dataset comprising AI-generated and real images. To accomplish this, pygoogle_image is used , with which images from Google are downloaded. Apart from Google Images, two more datasets are used. Then, a Convulutional Nueral Network based Classifier model was constructed and trained on the dataset containing around 1,00,000 images.\n",
        "\n",
        "#### First, let's import all the necessary libraries\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2 as cv\n",
        "from pygoogle_image import image as pi\n",
        "import random\n",
        "import PIL\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, models, Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# PART 1: DATA GATHERING & PREPROCESSING (BASELINE)\n",
        "# ====================================================================\n",
        "\n",
        "### Generating a Dataset\n",
        "\n",
        "##### Here, we used pygoogle_image library to download google images\n",
        "# Note: This cell is commented out as it only needs to be run once.\n",
        "# pi.download(keywords=\"ai generated images\", limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords=\"ai generated art\", limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords=\"ai generated characters\", limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords='stable diffusion', limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords='dalle2 generated images', limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords='midjourney', limit=100, directory='./ai_generated/')\n",
        "# pi.download(keywords='landscapes', limit=100, directory='./real/')\n",
        "# pi.download(keywords='cityscapes', limit=100, directory='./real/')\n",
        "# pi.download(keywords='animals', limit=100, directory='./real/')\n",
        "# pi.download(keywords='vehicles', limit=50, directory='./real/')\n",
        "# pi.download(keywords='traffic', limit=50, directory='./real/')\n",
        "# pi.download(keywords='offices', limit=50, directory='./real/')\n",
        "# pi.download(keywords='real food images', limit=50, directory='./real/')\n",
        "\n",
        "##### Since, the data collected from google images is not enought for training a model, we have used two more datasets\n",
        "#\n",
        "# 1. CIFAKE: Real and AI-Generated Synthetic Images\n",
        "# 2. Ai Generated Images | Images Created using Ai from Kaggle\n",
        "\n",
        "#### Now, we will be pre-processing the data\n",
        "# This cell loads the 100k images from the external datasets and saves them to pickle files.\n",
        "\n",
        "# data = \"./dataset_train/\"\n",
        "# categories = ['Real', 'AIGenerated']\n",
        "# img_size = 48\n",
        "# training_data = []\n",
        "\n",
        "# i = 0\n",
        "# for category in categories:\n",
        "#     path = os.path.join(data,category)\n",
        "#     classes = categories.index(category)\n",
        "#     for img in os.listdir(path):\n",
        "#         i = i + 1\n",
        "#         img_array = cv.imread(os.path.join(path,img))\n",
        "#         if img_array is None:\n",
        "#             continue\n",
        "#         new_array = cv.resize(img_array, (48,48))\n",
        "#         new_array = new_array/255\n",
        "#         training_data.append([new_array, classes])\n",
        "\n",
        "# random.shuffle(training_data)\n",
        "\n",
        "# X_train = []\n",
        "# y_train = []\n",
        "\n",
        "# for features, label in training_data:\n",
        "#     X_train.append(features)\n",
        "#     y_train.append(label)\n",
        "\n",
        "# X_train = np.array(X_train).reshape(-1, img_size, img_size, 3)\n",
        "# y_train = np.array(y_train)\n",
        "\n",
        "#### Storing X_train and y_train\n",
        "# pickle_out = open(\"X_train.pickle\", \"wb\")\n",
        "# pickle.dump(X_train, pickle_out, protocol=4)\n",
        "# pickle_out.close()\n",
        "\n",
        "# pickle_out = open(\"y_train.pickle\", \"wb\")\n",
        "# pickle.dump(y_train, pickle_out, protocol=4)\n",
        "# pickle_out.close()\n",
        "\n",
        "# ====================================================================\n",
        "# PART 2: BASELINE MODEL (TRAINED & SAVED)\n",
        "# ====================================================================\n",
        "\n",
        "# Load the preprocessed data from pickle files\n",
        "# (Assuming this data is already generated and present)\n",
        "pickle_in = open(\"X_train.pickle\", \"rb\")\n",
        "X_train = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "pickle_in = open(\"y_train.pickle\", \"rb\")\n",
        "y_train = pickle.load(pickle_in)\n",
        "pickle_in.close()\n",
        "\n",
        "print(f\"Loaded X_train shape: {X_train.shape}\")\n",
        "print(f\"Loaded y_train shape: {y_train.shape}\")\n",
        "\n",
        "## Creating the Baseline Model\n",
        "model_baseline = keras.Sequential([\n",
        "    keras.layers.Conv2D(32,(3,3), activation='relu', input_shape = (48,48,3)),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    keras.layers.Conv2D(64,(3,3), activation='relu'),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    keras.layers.Conv2D(128,(3,3), activation='relu'),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    keras.layers.Conv2D(256,(3,3), activation='relu'),\n",
        "    keras.layers.MaxPool2D((2,2)),\n",
        "    keras.layers.Dropout(0.2),\n",
        "\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_baseline.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_baseline.summary()\n",
        "\n",
        "## Training the Baseline Model\n",
        "# (Skipping this as the notebook shows it's already done and saved)\n",
        "# model_baseline.fit(X_train, y_train, epochs=15)\n",
        "# model_baseline.save(\"AIGeneratedModel.h5\")\n",
        "\n",
        "\n",
        "## Evaluating the Baseline Model\n",
        "print(\"Loading saved baseline model for testing...\")\n",
        "model_new = keras.models.load_model(\"AIGeneratedModel.h5\")\n",
        "\n",
        "# Load and process test data\n",
        "data_test = \"./dataset_test/\"\n",
        "categories_test = ['Real', 'AIGenerated']\n",
        "img_size = 48\n",
        "testing_data = []\n",
        "\n",
        "i = 0\n",
        "for category in categories_test:\n",
        "    path = os.path.join(data_test, category)\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"Test directory not found: {path}. Skipping evaluation.\")\n",
        "        break\n",
        "\n",
        "    classes = categories_test.index(category)\n",
        "    for img in os.listdir(path):\n",
        "        i = i + 1\n",
        "        img_array = cv.imread(os.path.join(path,img))\n",
        "        if img_array is None:\n",
        "            continue\n",
        "        new_array = cv.resize(img_array, (48,48))\n",
        "        new_array = new_array/255\n",
        "        testing_data.append([new_array, classes])\n",
        "\n",
        "if testing_data:\n",
        "    random.shuffle(testing_data)\n",
        "\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for features, label in testing_data:\n",
        "        X_test.append(features)\n",
        "        y_test.append(label)\n",
        "\n",
        "    X_test = np.array(X_test).reshape(-1, img_size, img_size, 3)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    print(\"Evaluating baseline model on test data...\")\n",
        "    model_new.evaluate(X_test, y_test)\n",
        "\n",
        "    y_pred = model_new.predict(X_test)\n",
        "    y_predicted = [1 if arr[0] > 0.5 else 0 for arr in y_pred]\n",
        "\n",
        "    print(\"\\nClassification Report (Baseline Model):\")\n",
        "    print(classification_report(y_test, y_predicted))\n",
        "\n",
        "\n",
        "## Testing (Demo Function)\n",
        "def find_out(path_img, model_to_use):\n",
        "    img_arr = cv.imread(path_img)\n",
        "    if img_arr is None:\n",
        "        print(f\"Error: Could not read image at {path_img}\")\n",
        "        return\n",
        "\n",
        "    plt.imshow(cv.cvtColor(img_arr, cv.COLOR_BGR2RGB)) # Convert BGR to RGB for plt\n",
        "    new_arr = cv.resize(img_arr, (48,48))\n",
        "    new_arr = new_arr/255\n",
        "    test = np.array(new_arr).reshape(-1, img_size, img_size, 3)\n",
        "\n",
        "    y = model_to_use.predict(test)\n",
        "\n",
        "    if y[0][0] <= 0.5:\n",
        "        print(\"Prediction: The given image is Real.\")\n",
        "    else:\n",
        "        print(\"Prediction: The given image is AI Generated.\")\n",
        "    plt.show()\n",
        "\n",
        "# Demo with baseline model\n",
        "print(\"\\n--- Baseline Model Demo ---\")\n",
        "path_img_real = './Testing/Real.jpeg'\n",
        "if os.path.exists(path_img_real):\n",
        "    find_out(path_img_real, model_new)\n",
        "else:\n",
        "    print(f\"Demo image not found: {path_img_real}\")\n",
        "\n",
        "path_img_ai = './Testing/AIGenerated.png'\n",
        "if os.path.exists(path_img_ai):\n",
        "    find_out(path_img_ai, model_new)\n",
        "else:\n",
        "    print(f\"Demo image not found: {path_img_ai}\")\n",
        "\n",
        "\n",
        "# ====================================================================\n",
        "# PART 3: UPDATED MODEL (DATA AUGMENTATION & BATCH NORM)\n",
        "# ====================================================================\n",
        "\n",
        "## 1. Introduce Data Augmentation\n",
        "# This loads X_train and y_train again, as they are needed for the generator.\n",
        "# (This step was missing in the original notebook's final cells)\n",
        "try:\n",
        "    pickle_in = open(\"X_train.pickle\", \"rb\")\n",
        "    X_train_aug = pickle.load(pickle_in)\n",
        "    pickle_in.close()\n",
        "\n",
        "    pickle_in = open(\"y_train.pickle\", \"rb\")\n",
        "    y_train_aug = pickle.load(pickle_in)\n",
        "    pickle_in.close()\n",
        "\n",
        "    print(\"\\nData loaded for augmentation.\")\n",
        "\n",
        "    # Instantiate ImageDataGenerator\n",
        "    datagen = ImageDataGenerator(\n",
        "        rotation_range=20,\n",
        "        width_shift_range=0.1,\n",
        "        height_shift_range=0.1,\n",
        "        zoom_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode='nearest'\n",
        "    )\n",
        "\n",
        "    # Fit the ImageDataGenerator\n",
        "    datagen.fit(X_train_aug)\n",
        "\n",
        "    # Create an augmented data generator\n",
        "    train_datagen = datagen.flow(X_train_aug, y_train_aug, batch_size=32)\n",
        "    print(\"ImageDataGenerator instantiated and fitted.\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"\\nWarning: X_train.pickle or y_train.pickle not found.\")\n",
        "    print(\"Skipping augmentation and training of the new model.\")\n",
        "    train_datagen = None\n",
        "\n",
        "\n",
        "## 2. Refine Model Architecture (with Batch Normalization)\n",
        "print(\"\\nDefining new model with Batch Normalization...\")\n",
        "model_updated = keras.Sequential([\n",
        "    layers.Conv2D(32,(3,3), input_shape = (48,48,3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPool2D((2,2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(64,(3,3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPool2D((2,2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(128,(3,3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPool2D((2,2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Conv2D(256,(3,3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "    layers.MaxPool2D((2,2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    layers.Dense(64),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Activation('relu'),\n",
        "\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_updated.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model_updated.summary()\n",
        "\n",
        "\n",
        "## 3. Train the Updated Model\n",
        "# This step trains the new model using the augmented data generator\n",
        "if train_datagen:\n",
        "    print(\"\\nTraining updated model with augmented data...\")\n",
        "    # Calculate steps_per_epoch\n",
        "    steps_per_epoch = len(X_train_aug) // 32  # 32 is the batch_size\n",
        "\n",
        "    history_updated = model_updated.fit(\n",
        "        train_datagen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=15  # You can increase epochs as augmentation reduces overfitting\n",
        "    )\n",
        "\n",
        "    print(\"Training of updated model complete.\")\n",
        "\n",
        "    # Save the new model\n",
        "    model_updated.save(\"AIGeneratedModel_Updated.h5\")\n",
        "    print(\"Updated model saved as AIGeneratedModel_Updated.h5\")\n",
        "\n",
        "    # (Optional) Evaluate the new model on the test set\n",
        "    if 'X_test' in locals():\n",
        "        print(\"\\nEvaluating updated model on test data...\")\n",
        "        model_updated.evaluate(X_test, y_test)\n",
        "\n",
        "        y_pred_updated = model_updated.predict(X_test)\n",
        "        y_predicted_updated = [1 if arr[0] > 0.5 else 0 for arr in y_pred_updated]\n",
        "\n",
        "        print(\"\\nClassification Report (Updated Model):\")\n",
        "        print(classification_report(y_test, y_predicted_updated))\n",
        "else:\n",
        "    print(\"\\nSkipping training of updated model as data was not loaded.\")"
      ]
    }
  ]
}